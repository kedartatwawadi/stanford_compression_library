"""
Simplified LZ77 encoder deriving ideas from: 
- LZ77 modern implementation: https://glinscott.github.io/lz/index.html 
- DEFLATE: https://www.rfc-editor.org/rfc/rfc1951 
- ZSTD: https://github.com/facebook/zstd/blob/dev/doc/zstd_compression_format.md

The encoder has 2 parameters: 
- min_match_length: this determines the minimum
allowed match length. The same length is also used for indexing into the dict.
Very low values lead to slow execution. In terms of compression, low values can
lead to suboptimal matches but high values can lead to missed matches. So there
is a tradeoff. Default is 6.
- max_num_matches_considered: Max number of matches considered at
a position to bound time complexity. The matches are considered in order of
recency and the longest match among the max_num_matches_considered is chosen.
Defaults to 64.

The algorithm: 

- keep a window that stores everything the encoder has seen (not reset across blocks
 unless reset called). 
- keep a dict that stores a map from tuples of length of min_match_length to the 
positions where the tuple is seen. 
- to find a match during parsing, we look up in the dict and then find the longest
match among the most recent max_num_matches_considered. If no match at a position, we
search for matches starting at next. We do this until we reach the end of the block
and don't find anything.
- the streams generated by parsing are as follows:
1. Literals - when no matches found for a position we put in literals. This can also include
the very end of the block which is unmatched.
2. LZ77 sequences - this is a triple of literal_counts, match_length and match_offset. The idea 
is that when we start looking for a match starting at pos_in_window, but only find a match starting
at match_start_pos, we set literal_counts to match_start_pos-pos_in_window. The match length and
offset have their usual meaning. Note that as in gzip/zstd we allow match_length to be greater than
or equal to the offset, denoting overlapping matches. Decoding this needs to be done a bit carefully
since a simple memcpy doesn't work. Finally, note that the sequences do not encode the last part
of the block that has only literals and no matches. After executing all the sequences, the decoder
simply copies over any leftover literals to the output.

Entropy encoding:
- Literals: encoded with Huffman based on empirical distribution. The counts are stored using
Elias Delta code.
- LZ77 sequences - we first process the match_length by subtracting min_match_length and match_offset
by subtracting 1. Then we concatenate the literal counts and the processed match lengths and offsets
and pass them all into an Elias Delta code.


Current limitations: 
1. During compression we use a naive match finding whose
   memory usage accumulates over time. We could implement the hash-chain algorithm 
   described at https://glinscott.github.io/lz/index.html#toc4.2.2.
2. During decompression we keep the entire past output buffer in memory. We
   could implement window-based scheme and implement shifting the window to
   reduce memory usage.
3. We implement greedy parsing that takes the longest match at a given position.
   Instead we could implement more optimal parsing which tries to skip some
   bytes as literals to find a longer match in the upcoming bytes.

Given the above, note that you could still reset the encoder in order after a
few blocks to limit memory usage.
"""

import argparse
from dataclasses import dataclass
import os
import tempfile
from typing import List, Tuple
from compressors.elias_delta_uint_coder import EliasDeltaUintDecoder, EliasDeltaUintEncoder
from compressors.huffman_coder import HuffmanDecoder, HuffmanEncoder
from core.data_block import DataBlock
from core.data_encoder_decoder import DataDecoder, DataEncoder
from core.data_stream import Uint8FileDataStream
from core.encoded_stream import EncodedBlockReader, EncodedBlockWriter
from core.prob_dist import ProbabilityDist
from utils.bitarray_utils import BitArray, bitarray_to_uint, uint_to_bitarray
from utils.test_utils import (
    create_random_binary_file,
    try_file_lossless_compression,
    try_lossless_compression,
)

ENCODED_BLOCK_SIZE_HEADER_BITS = (
    32  # number of bits used to put a header consisting of encoded block size
)
DEFAULT_MIN_MATCH_LEN = 6
DEFAULT_MAX_NUM_MATCHES_CONSIDERED = 64


@dataclass
class LZ77Sequence:
    """LZ77Sequence that determines a series of operations during decompression.
    - First copy `literal_count` literal characters to output.
    - Next copy `match_length` characters from `match_offset` back in output to the output.
    """

    literal_count: int = (0,)
    match_length: int = (0,)
    match_offset: int = (0,)


class LZ77Encoder(DataEncoder):
    def __init__(
        self,
        min_match_length: int = DEFAULT_MIN_MATCH_LEN,
        max_num_matches_considered: int = DEFAULT_MAX_NUM_MATCHES_CONSIDERED,
        window_initialization: List = None,
    ):
        """LZ77Encoder. See module documentation for details.

        Args:
            min_match_length (int, optional): Minimum match length. Defaults to 6.
            max_num_matches_considered (int, optional): Max number of matches considered
                at a position to bound time complexity. The matches are considered
                in order of recency and the longest match among the max_num_matches_considered
                is chosen. Defaults to 64.
            window_initialization (List, optional): initialize window (this is like side information
                or dictionary in zstd parlance). The same initialization should be used for the decoder.

        """
        self.min_match_length = min_match_length
        self.max_num_matches_considered = max_num_matches_considered
        self.window = []
        self.dict = (
            {}
        )  # map from substr of length min_match_length to list of positions where it occurs
        self.window_indexed_till = 0  # pointer telling up to what point the window has been indexed
        # if window_indexed_till = 100, that means all substrings starting at 0,1,2,...,100-min_match_length+1
        # have been indexed
        if window_initialization is not None:
            self.window = list(window_initialization)
            self.index_window_upto_pos(len(self.window))

    def reset(self):
        self.window = []
        self.dict = {}
        self.window_indexed_till = 0

    def insert_substring_into_dict(self, substr: Tuple, start_pos: int):
        """Insert substring into the dict (mapping substring to positions where
        it occurs in the window).

        Args:
            substr (Tuple): tuple of length min_match_length
            start_pos (int): position of substr in window
        """
        if substr in self.dict:
            self.dict[substr].append(start_pos)
        else:
            self.dict[substr] = [start_pos]

    def index_window_upto_pos(self, end_pos: int):
        """Index all tuples of min_match_length in self.window[:end_pos]
        into the dict. The last tuple to be indexed will start at end_pos-min_match_length+1
        This uses self.window_indexed_till to ensure we do not reindex things already indexed.

        Args:
            end_pos (int): end position in window to index
        """
        for end_pos_substr in range(self.window_indexed_till, end_pos + 1):
            start_pos_substr = end_pos_substr - self.min_match_length
            if start_pos_substr < 0:
                continue
            substr = tuple(self.window[start_pos_substr:end_pos_substr])
            self.insert_substring_into_dict(substr, start_pos_substr)
        self.window_indexed_till = end_pos + 1

    def encode_lz77_sequences(self, lz77_sequences: LZ77Sequence):
        """Perform entropy encoding of the LZ77 sequences and return the encoded bitarray.

        We simply use the Elias Delta code for encoding all three, but first we preprocess a bit to
        optimize the encoding.

        Args:
            encoded_bitarray (BitArray): encoded bit array
        """
        literal_counts = [l.literal_count for l in lz77_sequences]
        # subtract min_match_length from match_length
        match_lengths_processed = [l.match_length - self.min_match_length for l in lz77_sequences]
        # subtract 1 from match offset because it's at least 1
        match_offsets_processed = [l.match_offset - 1 for l in lz77_sequences]
        # Combine all streams into one and then apply Elias Delta.
        # first encode the min_match_length (needed because we subtract it from match_lengths)
        combined = (
            [self.min_match_length]
            + literal_counts
            + match_lengths_processed
            + match_offsets_processed
        )
        combined_encoding = EliasDeltaUintEncoder().encode_block(DataBlock(combined))
        return (
            uint_to_bitarray(len(combined_encoding), ENCODED_BLOCK_SIZE_HEADER_BITS)
            + combined_encoding
        )

    def encode_literals(self, literals: List):
        """Perform entropy encoding of the literals and return the encoded bitarray.

        We apply Huffman coding for the literals and also store the counts to enable the
        decoder to construct the same Huffman tree.

        Args:
            encoded_bitarray (BitArray): encoded bit array
        """
        # first encode the literals with empirical Huffman code
        counts = DataBlock(literals).get_counts()
        if len(counts) > 0:
            prob_dict = ProbabilityDist.normalize_prob_dict(counts).prob_dict
            # let's sort prob_dist by the alphabet to make sure we get exact same ordering
            # during decompression as well, which makes sure it's the same Huffman tree!
            prob_dist_sorted = ProbabilityDist({i: prob_dict[i] for i in sorted(prob_dict)})

            literals_encoding = HuffmanEncoder(prob_dist_sorted).encode_block(DataBlock(literals))

            # Now encode the counts using Elias Delta code.
            # We first generate a list of counts (of length 256) and then apply Elias Delta.
            # For any bytes not seen in the data, set count to 0.
            for i in range(256):
                if i not in counts:
                    counts[i] = 0
            counts_list = [counts[i] for i in range(256)]

            counts_encoding = EliasDeltaUintEncoder().encode_block(DataBlock(counts_list))
            # combine everything into a single bitarray
            return (
                uint_to_bitarray(len(counts_encoding), ENCODED_BLOCK_SIZE_HEADER_BITS)
                + counts_encoding
                + uint_to_bitarray(len(literals_encoding), ENCODED_BLOCK_SIZE_HEADER_BITS)
                + literals_encoding
            )
        else:
            # if no counts (i.e., no literals) just transmit 0
            return uint_to_bitarray(0, ENCODED_BLOCK_SIZE_HEADER_BITS)

    def find_match_length(self, start_pos_1: int, start_pos_2: int):
        """Find the match length of window starting from start_pos_1 and start_pos_2.
           That is, largest match_length s.t.
           window[start_pos_1:start_pos_1+match_length]==window[start_pos_2:start_pos_2+match_length]
           Note that matching sections are allowed to overlap

        Args:
            start_pos_1 (int)
            start_pos_2 (int)

        Returns:
            int: match length
        """
        match_length = 0
        while start_pos_1 + match_length < len(self.window) and start_pos_2 + match_length < len(
            self.window
        ):
            if self.window[start_pos_1 + match_length] != self.window[start_pos_2 + match_length]:
                break
            else:
                match_length += 1
        return match_length

    def lz77_parse_generate_sequences(self, data_block: DataBlock):
        """Parse data using LZ77 and returns the LZ77 sequences and literals.

        Updates the window accordingly.

        Args:
            data_list (list of bytes (int)): input data
        """
        lz77_sequences = []
        literals = []

        pos_in_window = len(self.window)

        # put the entire data block in the window at once, we will find matches later
        self.window += data_block.data_list

        # now go over the window starting at pos_in_window and try to find matches
        # in the past
        # We keep going until we can't find a match anymore
        while True:
            match_start_pos = pos_in_window
            match_found = False
            for match_start_pos in range(
                pos_in_window, len(self.window) - self.min_match_length + 1
            ):
                match_substr = tuple(
                    self.window[match_start_pos : match_start_pos + self.min_match_length]
                )
                if match_substr not in self.dict:
                    self.index_window_upto_pos(match_start_pos + 1)
                    continue
                else:
                    candidate_match_positions = self.dict[match_substr]
                    best_match_pos = None
                    best_match_length = 0
                    num_candidates_considered = 0
                    # iterate over candidate_match_positions in reverse order
                    # we basically want to look at max_num_matches_considered most recent matches
                    for candidate_match_pos in reversed(candidate_match_positions):
                        match_len = self.find_match_length(candidate_match_pos, match_start_pos)
                        assert match_len >= self.min_match_length
                        if match_len > best_match_length:
                            best_match_length = match_len
                            best_match_pos = candidate_match_pos
                            match_found = True
                        num_candidates_considered += 1
                        if num_candidates_considered == self.max_num_matches_considered:
                            # only consider max_num_matches_considered to limit complexity
                            break
                if match_found:
                    break
                else:
                    self.index_window_upto_pos(match_start_pos + 1)

            if not match_found:
                # no match found anywhere so put everything else as a literal and break
                literals += self.window[pos_in_window:]
                # make sure entire window is indexed
                self.index_window_upto_pos(len(self.window))
                break
            else:
                # match was found so we appropriately insert into literals and sequences
                # first put part from pos_in_window to match_start_pos in literals
                literal_count = match_start_pos - pos_in_window
                literals += self.window[pos_in_window:match_start_pos]
                # compute the offset
                match_offset = match_start_pos - best_match_pos
                lz77_sequences.append(LZ77Sequence(literal_count, best_match_length, match_offset))
                match_end_pos = match_start_pos + best_match_length
                # index the covered portion into the dict
                self.index_window_upto_pos(match_end_pos)
                # update position in window
                pos_in_window = match_end_pos

        return lz77_sequences, literals

    def encode_block(self, data_block: DataBlock):
        # first do lz77 parsing
        lz77_sequences, literals = self.lz77_parse_generate_sequences(data_block)
        lz77_sequences_encoding = self.encode_lz77_sequences(lz77_sequences)
        literals_encoding = self.encode_literals(literals)
        return lz77_sequences_encoding + literals_encoding

    def encode_file(self, input_file_path: str, encoded_file_path: str, block_size: int = 10000):
        """utility wrapper around the encode function using Uint8FileDataStream

        Args:
            input_file_path (str): path of the input file
            encoded_file_path (str): path of the encoded binary file
            block_size (int): choose the block size to be used to call the encode function
        """
        # call the encode function and write to the binary file
        with Uint8FileDataStream(input_file_path, "rb") as fds:
            with EncodedBlockWriter(encoded_file_path) as writer:
                self.encode(fds, block_size=block_size, encode_writer=writer)


class LZ77Decoder(DataDecoder):
    def __init__(self, window_initialization: List = None):
        """Initialize LZ77 decoder.

        Args:
            window_initialization (List, optional): initialize window (this is like side information
                or dictionary in zstd parlance). The same initialization should be used as in encoder.
        """
        self.window = []
        if window_initialization is not None:
            self.window = list(window_initialization)

    def decode_lz77_sequences(self, encoded_bitarray: BitArray):
        """Perform entropy decoding of the LZ77 sequences and return the decoded sequences
        and the number of bits consumed.

        Args:
            encoded_bitarray (BitArray): encoded bit array
        """
        # first get the encoded block size so we can pick out the block for decoding
        encoded_block_size_bitarray = encoded_bitarray[:ENCODED_BLOCK_SIZE_HEADER_BITS]
        encoded_block_size = bitarray_to_uint(encoded_block_size_bitarray)
        encoded_bitarray = encoded_bitarray[
            ENCODED_BLOCK_SIZE_HEADER_BITS : ENCODED_BLOCK_SIZE_HEADER_BITS + encoded_block_size
        ]
        combined_decoded, num_bits_consumed_encoding = EliasDeltaUintDecoder().decode_block(
            encoded_bitarray
        )
        combined_decoded = combined_decoded.data_list
        assert num_bits_consumed_encoding == encoded_block_size
        num_bits_consumed = ENCODED_BLOCK_SIZE_HEADER_BITS + encoded_block_size
        assert (len(combined_decoded) - 1) % 3 == 0
        num_sequences = (len(combined_decoded) - 1) // 3
        min_match_length = combined_decoded[0]
        literal_counts = combined_decoded[1 : 1 + num_sequences]
        match_lengths = [
            l + min_match_length
            for l in combined_decoded[1 + num_sequences : 1 + 2 * num_sequences]
        ]
        match_offsets = [
            l + 1 for l in combined_decoded[1 + 2 * num_sequences : 1 + 3 * num_sequences]
        ]
        lz77_sequences = [
            LZ77Sequence(l[0], l[1], l[2])
            for l in zip(literal_counts, match_lengths, match_offsets)
        ]
        return lz77_sequences, num_bits_consumed

    def decode_literals(self, encoded_bitarray: BitArray):
        """Perform entropy decoding of the literals and return the literals
        and the number of bits consumed.

        Args:
            encoded_bitarray (BitArray): encoded bit array
        """
        num_bits_consumed = 0
        counts_encoding_size = bitarray_to_uint(encoded_bitarray[:ENCODED_BLOCK_SIZE_HEADER_BITS])
        num_bits_consumed += ENCODED_BLOCK_SIZE_HEADER_BITS
        if counts_encoding_size == 0:
            return [], num_bits_consumed  # no literals
        counts, num_bits_consumed_counts = EliasDeltaUintDecoder().decode_block(
            encoded_bitarray[num_bits_consumed : num_bits_consumed + counts_encoding_size]
        )
        counts = counts.data_list
        assert counts_encoding_size == num_bits_consumed_counts
        num_bits_consumed += counts_encoding_size
        # generate the probability distribution from the counts
        counts = {i: counts[i] for i in range(256) if counts[i] > 0}
        prob_dist = ProbabilityDist.normalize_prob_dict(counts)

        # load the encoding of the literals
        literals_huffman_encoding_size = bitarray_to_uint(
            encoded_bitarray[num_bits_consumed : num_bits_consumed + ENCODED_BLOCK_SIZE_HEADER_BITS]
        )
        num_bits_consumed += ENCODED_BLOCK_SIZE_HEADER_BITS
        decoded_literals, num_bits_consumed_literals = HuffmanDecoder(prob_dist).decode_block(
            encoded_bitarray[num_bits_consumed : num_bits_consumed + literals_huffman_encoding_size]
        )
        assert literals_huffman_encoding_size == num_bits_consumed_literals
        num_bits_consumed += literals_huffman_encoding_size
        return decoded_literals.data_list, num_bits_consumed

    def execute_lz77_sequences(self, literals: List, lz77_sequences: List[LZ77Sequence]):
        """Executes the LZ77 sequences and the literals and returns the decoded bytes.

        Updates the window accordingly.
        """
        window_len_before = len(self.window)
        pos_in_literals = 0
        for seq in lz77_sequences:
            # first copy over the literals
            self.window += literals[pos_in_literals : pos_in_literals + seq.literal_count]
            pos_in_literals += seq.literal_count
            # now copy the match
            if seq.match_length < seq.match_offset:
                # if the match length is not bigger than the offset a normal copy works!
                self.window += self.window[-seq.match_offset : -seq.match_offset + seq.match_length]
            else:
                # the match length exceeds the offset, so we need to copy byte by byte
                # (since the entire buffer to copy is not yet filled)
                for _ in range(seq.match_length):
                    self.window.append(self.window[-seq.match_offset])

        # copy over any leftover literals
        self.window += literals[pos_in_literals:]

        return self.window[window_len_before:]

    def decode_block(self, encoded_bitarray: BitArray):
        # first entropy decode the lz77 sequences and the literals
        lz77_sequences, num_bits_consumed_sequences = self.decode_lz77_sequences(encoded_bitarray)
        encoded_bitarray = encoded_bitarray[num_bits_consumed_sequences:]
        literals, num_bits_consumed_literals = self.decode_literals(encoded_bitarray)
        num_bits_consumed = num_bits_consumed_sequences + num_bits_consumed_literals

        # now execute the sequences to decode
        decoded_block = DataBlock(self.execute_lz77_sequences(literals, lz77_sequences))
        return decoded_block, num_bits_consumed

    def decode_file(self, encoded_file_path: str, output_file_path: str):
        """utility wrapper around the decode function using Uint8FileDataStream

        Args:
            encoded_file_path (str): input binary file
            output_file_path (str): output (text) file to which decoded data is written
        """

        # decode data and write output to a text file
        with EncodedBlockReader(encoded_file_path) as reader:
            with Uint8FileDataStream(output_file_path, "wb") as fds:
                self.decode(reader, fds)


def test_lz77_encode_decode():
    initial_window = [0, 0, 1, 1, 1]
    # see the test_lz77_sequence_generation for the parsing of this sequence
    data_list = [
        1,
        1,
        1,
        1,
        0,
        0,
        1,
        1,
        1,
        255,
        254,
        255,
        254,
        255,
        254,
        255,
        2,
        0,
        0,
        1,
        1,
        1,
        1,
        44,
    ]
    data_block = DataBlock(data_list)

    for min_match_length in [1, 2, 3, 4, 5]:
        for max_num_matches_considered in [0, 1, 5]:
            encoder = LZ77Encoder(
                min_match_length, max_num_matches_considered, window_initialization=initial_window
            )
            decoder = LZ77Decoder(window_initialization=initial_window)
            is_lossless, _, _ = try_lossless_compression(
                data_block, encoder, decoder, add_extra_bits_to_encoder_output=True
            )
    assert is_lossless


def test_lz77_sequence_generation():
    """
    Test that lz77 produces expected sequences
    Also test behavior across blocks both when we reset and when we don't
    """
    min_match_len = 3
    initial_window = [0, 0, 1, 1, 1]
    encoder = LZ77Encoder(min_match_length=min_match_len, window_initialization=initial_window)

    data_list = [
        1,
        1,
        1,
        1,
        0,
        0,
        1,
        1,
        1,
        255,
        254,
        255,
        254,
        255,
        254,
        255,
        2,
        0,
        0,
        1,
        1,
        1,
        1,
        44,
    ]
    data_block = DataBlock(data_list)

    # matches here are (first one is overlapping, last one picks longer match which is not the most recent match for 3-tuple):
    # 0, 0, [1, 1, 1, [1,] 1, 1, 1] 0, 0, 1, 1, 1, 255, 254, 255, 254, 255, 254, 255, 2, 0, 0, 1, 1, 1, 1, 44
    # [0, 0, 1, 1, 1,] 1, 1, 1, [0, 0, 1, 1, 1,] 255, 254, 255, 254, 255, 254, 255, 2, 0, 0, 1, 1, 1, 1, 44
    # 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, [255, 254, 255,] 254, [255, 254, 255,] 2, 0, 0, 1, 1, 1, 1, 44
    # [0, 0, 1, 1, 1, 1,] 1, 1, 0, 0, 1, 1, 1, 255, 254, 255, 254, 255, 254, 255, 2, [0, 0, 1, 1, 1, 1], 44

    expected_lits = [255, 254, 255, 254, 2, 44]
    expected_seqs = [
        LZ77Sequence(0, 4, 3),
        LZ77Sequence(0, 5, 9),
        LZ77Sequence(4, 3, 4),
        LZ77Sequence(1, 6, 22),
    ]
    seqs, lits = encoder.lz77_parse_generate_sequences(data_block)

    assert encoder.window == initial_window + data_list
    assert sum(len(v) for v in encoder.dict.values()) == len(encoder.window) - min_match_len + 1
    assert lits == expected_lits
    assert seqs == expected_seqs

    # encode another block which is copy of first and see that we get just one match
    seqs, lits = encoder.lz77_parse_generate_sequences(data_block)
    assert encoder.window == initial_window + data_list * 2
    assert (
        sum(len(v) for v in encoder.dict.values()) == len(encoder.window) - min_match_len + 1
    )  # subtract 2 since min_match_len is 3
    assert lits == []
    assert seqs == [LZ77Sequence(0, len(data_list), len(data_list))]

    # now reset encoder and verify that after encoding we get results that we expect without initial window

    # matches:
    # 1, [1, 1, 1,] 0, 0, [1, 1, 1,] 255, 254, 255, 254, 255, 254, 255, 2, 0, 0, 1, 1, 1, 1, 44
    # 1, 1, 1, 1, 0, 0, 1, 1, 1, [255, 254, 255,] 254, [255, 254, 255,] 2, 0, 0, 1, 1, 1, 1, 44
    # 1, 1, 1, 1, [0, 0, 1, 1, 1,] 255, 254, 255, 254, 255, 254, 255, 2, [0, 0, 1, 1, 1,] 1, 44
    encoder.reset()
    expected_lits = [1, 1, 1, 1, 0, 0, 255, 254, 255, 254, 2, 1, 44]
    expected_seqs = [LZ77Sequence(6, 3, 5), LZ77Sequence(4, 3, 4), LZ77Sequence(1, 5, 13)]
    seqs, lits = encoder.lz77_parse_generate_sequences(data_block)

    assert encoder.window == data_list
    assert sum(len(v) for v in encoder.dict.values()) == len(data_list) - min_match_len + 1
    assert lits == expected_lits
    assert seqs == expected_seqs


def test_lz77_multiblock_file_encode_decode():
    """full test for LZ77Encoder and LZ77Decoder

    - create a sample file
    - encode the file using LZ77Encoder
    - perform decoding and check if the compression was lossless

    """
    initial_window = [44, 45, 46] * 5
    # define encoder, decoder
    encoder = LZ77Encoder(window_initialization=initial_window)
    decoder = LZ77Decoder(window_initialization=initial_window)

    with tempfile.TemporaryDirectory() as tmpdirname:
        # create a file with some random data
        input_file_path = os.path.join(tmpdirname, "inp_file.txt")
        create_random_binary_file(
            input_file_path,
            file_size=500,
            prob_dist=ProbabilityDist({44: 0.5, 45: 0.25, 46: 0.2, 255: 0.05}),
        )

        # test lossless compression
        assert try_file_lossless_compression(
            input_file_path, encoder, decoder, encode_block_size=1000
        )


# Provide a simple CLI interface below for convenient experimentation
parser = argparse.ArgumentParser()
parser.add_argument("-d", "--decompress", help="decompress", action="store_true")
parser.add_argument("-i", "--input", help="input file", required=True, type=str)
parser.add_argument("-o", "--output", help="output file", required=True, type=str)
parser.add_argument(
    "-w", "--window_init", help="initialize window from file (like zstd dictionary)", type=str
)

# constants
BLOCKSIZE = 100_000  # encode in 100 KB blocks

if __name__ == "__main__":
    args = parser.parse_args()

    window_initialization = None
    if args.window_init is not None:
        with open(args.window_init, "rb") as f:
            window_initialization = list(f.read())

    if args.decompress:
        decoder = LZ77Decoder(window_initialization=window_initialization)
        decoder.decode_file(args.input, args.output)
    else:
        encoder = LZ77Encoder(window_initialization=window_initialization)
        encoder.encode_file(args.input, args.output, block_size=BLOCKSIZE)
